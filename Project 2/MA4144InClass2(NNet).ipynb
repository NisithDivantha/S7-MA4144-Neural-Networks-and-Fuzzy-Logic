{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56157fb7-11e7-4151-8a9c-e95c4a844924",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# <center>Feedforward and Backpropagation</center>\n",
    "## <center>Inclass Project 2 - MA4144</center>\n",
    "\n",
    "This project contains 12 tasks/questions to be completed, some require written answers. Open a markdown cell below the respective question that require written answers and provide (type) your answers. Questions that required written answers are given in blue fonts. Almost all written questions are open ended, they do not have a correct or wrong answer. You are free to give your opinions, but please provide related answers within the context.\n",
    "\n",
    "After finishing project run the entire notebook once and **save the notebook as a pdf** (File menu -> Save and Export Notebook As -> PDF). You are **required to upload this PDF on moodle and also the ipynb notebook file as well**.\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37c45f68-8516-4bd3-80b3-fa38c9fc01bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Outline of the project\n",
    "\n",
    "The aim of the project is to build a Multi Layer perceptron (MLP) model from scratch for binary classification. That is given an input $x$ output the associated class label $0$ or $1$.\n",
    "\n",
    "In particular, we will classify images of handwritten digits ($0, 1, 2, \\cdots, 9$). For example, given a set of handwritten digit images that only contain two digits (Eg: $1$ and $5$) the model will classify the images based on the written digit.\n",
    "\n",
    "For this we will use the MNIST dataset (collection of $28 \\times 28$ images of handwritten digits) - you can find additional information about MNIST [here](https://en.wikipedia.org/wiki/MNIST_database).\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png\" width=\"250\">\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a0988-42a0-4e8f-abf5-2bc3ddf18304",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Use the below cell to use any include any imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac8649a-c1fe-4236-8ebd-ce3faa43ee3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c6025-f0e5-440e-ab25-fb22393f186c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Section 1: Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a0c01b-d4ce-47c6-ad39-80ea85a59543",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset as training and testing, then print out the shapes of the data matrices.\n",
    "#The training data will be provided to you.\n",
    "\n",
    "data = np.load('train_mnist.npz')\n",
    "train_X = data['x']\n",
    "train_y = data['y']\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1135bf15-9a80-4d14-8fb4-e30c9e53a4cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q1.** In the following cell write code to display $5$ random images in train_X and it's corresponding label in train_y. Each time it is run, you should get a different set of images. The imshow function in the matplotlib library could be useful. Display them as [grayscale images](https://en.wikipedia.org/wiki/Grayscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad09af0-90d7-4590-922e-d114755455a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO Code to display 5 random handritten images from train_X and corresponting labels from train_y\n",
    "\n",
    "random_indices = np.random.choice(train_X.shape[0], size=5, replace=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(train_X[idx], cmap='gray')\n",
    "    plt.title(f\"Label: {train_y[idx]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fd94a-370c-468b-ba0f-01c2506d4810",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q2.** Given two digits $d_1$ and $d_2$, both between $0$ and $9$, in the following cell fill in the function body to extract all the samples corresponding to $d_1$ or $d_2$ only, from the dataset $X$ and labels $y$. You can use the labels $y$ to filter the dataset. Assume that the label for the $i$th image $X[i]$ in $X$ is given by $y[i]$. The function should return the extracted samples $X_{extracted}$ and corresponding labels $y_{extracted}$. Avoid using for loops as much as possible, infact you do not need any for loops. numpy.where function should be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cd468f8-f28e-405d-aa0a-ac97470615f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_digits(X, y, d1, d2):\n",
    "\n",
    "    assert d1 in range(0, 10), \"d1 should be a number between 0 and 9 inclusive\"\n",
    "    assert d2 in range(0, 10), \"d2 should be a number between 0 and 9 inclusive\"\n",
    "    \n",
    "    #TODO\n",
    "    indices = np.where((y == d1) | (y == d2))[0]\n",
    "    X_extracted = X[indices]\n",
    "    y_extracted = y[indices]\n",
    "\n",
    "    return (X_extracted, y_extracted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29123736-f5a7-4981-afd8-cdb69bae3f7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q3.** Both the training dataset is a 3 dimensional numpy array, each image occupies 2 dimensions. For convenience of processing data we usually comvert each $28 \\times 28$ image matrix to a vector with $784$ entries. We call this process **vectorize images**.\n",
    "\n",
    "Once we vectorize the images, the vectorized data set would be structured as follows: $i$th row will correspond to a single image and $j$th column will correspond to the $j$th pixel value of each vectorized image. However going along with the convention we discussed in the lecture, the input to the MLP model will require that the columns correspond to individual images. Hence we also require a transpose of the vectorized results.\n",
    "\n",
    "The pixel values in the images will range from $0$ to $255$. Normalize the pixel values between $0$ and $1$, by dividing each pixel value of each image by the maximum pixel value of that image. Simply divide each column of the resulting matrix above by the max of each column. \n",
    "\n",
    "<center><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTdN_8m9FEqjqAB07obTmB6gNc7S2rSoGBYaA&s\"></center>\n",
    "\n",
    "Given a dataset $X$ of size $N \\times 28 \\times 28$, in the following cell fill in the function to do the following in order;\n",
    "1. Vectorize the dataset resulting in dataset of size $N \\times 784$.\n",
    "2. Transpose the vectorized result.\n",
    "3. Normalize the pixel values of each image.\n",
    "4. Finally return the vectorized, transposed and normalized dataset $X_{transformed}$.\n",
    "\n",
    "Again, avoid for loops, functions such as numpy.reshape, numpy.max etc should be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb50027a-5a36-4279-9972-a2bf449bef3d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorize_images(X):\n",
    "\n",
    "    #TODO\n",
    "    X_vectorized = X.reshape(X.shape[0], -1)\n",
    "    X_vectorized = X_vectorized.T\n",
    "    X_vectorized = X_vectorized / 255.0\n",
    "\n",
    "    return(X_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc428f8-0eec-4eb2-9df2-53cd924c9c9d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q4.** In the following cell write code to;\n",
    "\n",
    "1. Extract images of the digits $d_1 = 1$ and $d_2 = 5$ with their corresponding labels for the training set (train_X, train_y).\n",
    "2. Then vectorize the data, tranpose the result and normlize the images.\n",
    "3. Store the results after the final transformations in numpy arrays train_X_1_5, train_y_1_5.\n",
    "4. Our MLP will output only class labels $0$ and $1$ (not $1$ and $5$), so create numpy arrays to store the class labels as follows:\n",
    "   $d_1 = 1$ -> class label = 0 and $d_2 = 5$ -> class label = 1. Store them in an array named train_y_1_5.\n",
    "\n",
    "Use the above functions you implemented above to complete this task. In addtion, numpy.where could be useful. Avoid for loops as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f1eea24-60a5-48ce-829a-67d61645078d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Extract and organize the training data as described above.\n",
    "#Here you will be using the functions you implemented above appropriately\n",
    "\n",
    "X_1_5, y_1_5 = extract_digits(train_X, train_y, 1, 5)\n",
    "train_X_1_5 = vectorize_images(X_1_5)\n",
    "train_y_1_5 = np.where(y_1_5 == 1, 0, 1)  # Map 1 to 0 and 5 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d99382b-43b7-4375-8825-4b7d267b6dac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Section 2: Implementing MLP from scratch with training algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864b18e-cf2e-4da6-a3df-20b8bd84633f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now we will implement code to build a customizable MLP model. The hidden layers will have the **Relu activation function** and the final output layer will have **Sigmoid activation function**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247c43d-7e37-4e72-9232-5e01898c39a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q5.** Recall the following about the activation functions:\n",
    "1. Sigmoid activation: $y = \\sigma(z) = \\frac{1}{1 + e^{-z}}$.\n",
    "2. Derivative of Sigmoid: $y' = \\sigma'(z) = \\sigma(z) (1 - \\sigma(z)) = y(1-y)$\n",
    "3. ReLu activation: $y = ReLu(z) = max(0, z)$\n",
    "4. Derivative of ReLu: $y' = ReLu'(z) = \\begin{cases} 0 \\; \\textrm{if } z < 0 \\\\ 1 \\; \\textrm{otherwise} \\end{cases} = \\begin{cases} 0 \\; \\textrm{if } y = 0 \\\\ 1 \\; \\textrm{otherwise} \\end{cases}$\n",
    "\n",
    "In the following cell implement the functions to compute activation functions Sigmoid and ReLu given $z$ and derivatives of the Sigmoid and ReLu activation functions given $y$. Note that, in the implementation, the derivative functions should actually accept $y$ as the input not $z$.\n",
    "\n",
    "In practice the input will not be just single numbers, but matrices. So functions or derivatives should be applied elementwise on matrices. Again avoid for loops, use the power of numpy arrays - search for numpy's capability of doing elementwise computations.\n",
    "\n",
    "Important: When implementing the sigmoid function make sure you handle overflows due to $e^{-z}$ being too large. To avoid you can choose to set the sigmoid value to 'the certain appropriate value' if $z$ is less than a certain good enough negative threshold. If you do not handle overflows, the entire result will be useless since the MLP will just output Nan (not a number) for every input at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac42dde5-2b7d-4fc2-a5bc-4d396822e995",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-26cf7a0bbe0f6dd9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "\n",
    "    #TODO\n",
    "    sigma = 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    return(sigma)\n",
    "\n",
    "def deriv_sigmoid(Y):\n",
    "\n",
    "    #TODO\n",
    "    sigma_prime = Y * (1 - Y)\n",
    "    return(sigma_prime)\n",
    "    \n",
    "\n",
    "def ReLu(Z):\n",
    "\n",
    "    #TODO\n",
    "    relu = np.maximum(0, Z)\n",
    "\n",
    "    return(relu)\n",
    "\n",
    "def deriv_ReLu(Y):\n",
    "\n",
    "    #TODO\n",
    "    relu_prime = np.where(Y > 0, 1, 0)\n",
    "    \n",
    "    return(relu_prime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca012e-5c82-4689-8170-28f6110cc9b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q6.** The following piece of code defines a simple MLP architecture as a Python class and subsequent initialization of a MLP model. <font color='blue'>Certain lines of code contains commented line numbers. Write a short sentence for each such line explaining its purpose. Feel free to refer to the lecture notes or any resources to answers these question. In addition, explain what the Y, Z, W variables refer to and their purpose</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7985eb85-6a89-4d01-b6cf-d58cb8b4d23f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NNet:\n",
    "    def __init__(self, input_size = 784, output_size = 1, batch_size = 1000, hidden_layers = [500, 250, 50]):\n",
    "        self.Y = []\n",
    "        self.Z = []\n",
    "        self.W = []\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        layers = [input_size] + hidden_layers + [output_size]\n",
    "        L = len(hidden_layers) + 1\n",
    "    \n",
    "        for i in range(1, L + 1):\n",
    "            self.Y.append(np.zeros((layers[i], batch_size)))                        #line1\n",
    "            self.Z.append(np.zeros((layers[i], batch_size)))                        #Line2\n",
    "            self.W.append(2*(np.random.rand(layers[i], layers[i-1] + 1) - 0.5))     #Line3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854a0e4-9b83-4f52-8b59-3e64af141209",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Questions** (write answers in the cell below as strings)\n",
    "\n",
    "(i) What does the Y, Z, W variables refer to and their purpose?\n",
    "\n",
    "(ii) Line1: Explanation\n",
    "\n",
    "(iii) Line2: Explanation\n",
    "\n",
    "(iv) Line3: Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc9bf0a2-3bae-482d-8808-fe0085d0e55a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call here. Maybe you meant '==' instead of '='? (2908382141.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mans_(i) = \"Y, Z, and W are lists used to store the activations (Y), pre-activations (Z), and weights (W) for each layer of the neural network. Y holds the output of each layer after applying the activation function, Z holds the linear combination of inputs and weights before activation, and W holds the weight matrices (including bias) for each layer.\"\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m cannot assign to function call here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "#Write answers here as strings:\n",
    "\n",
    "ans_(i) = \"Y, Z, and W are lists used to store the activations (Y), pre-activations (Z), and weights (W) for each layer of the neural network. Y holds the output of each layer after applying the activation function, Z holds the linear combination of inputs and weights before activation, and W holds the weight matrices (including bias) for each layer.\"\n",
    "ans_(ii) = \"Line1 initializes the activations Y for each layer as zero matrices of appropriate shape, to store the output of each layer during forward propagation.\"\n",
    "ans_(iii) = \"Line2 initializes the pre-activations Z for each layer as zero matrices of appropriate shape, to store the linear combinations of inputs and weights before activation.\"\n",
    "ans_(iv) = \"Line3 initializes the weights W for each layer with small random values in the range [-1, 1], including an extra column for the bias term, to break symmetry and allow for effective learning.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee71406-c0f7-4114-9d9c-bf25fe6d208a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q7.** Now we will implement the feedforward algorithm. Recall from the lectures that for each layer $l$ there is input $Y^{(l-1)}$ from the previous layer if $l > 1$ and input data $X$ if $l = 1$. Then we compute $Z^{(l)}$ using the weight matrix $W^{(l)}$ as follows from matrix multiplication:\n",
    "\n",
    "$Z^{(l)} = W^{(l)} Y^{(l-1)}$\n",
    "\n",
    "Make sure that during multiplication you add an additional row of one's to $Y^{(l-1)}$ to accommodate the bias term (concatenate the row of ones as the last row to be consistent with the grader). However, the rows of ones should not permanently remain on $Y^{(l-1)}$. <font color='blue'>Explain what the bias term is and how adding a row of one's help with the bias terms.</font> The weight matrices are initialised to afford this extra bias term, so no change to either $Z^{(l)}$ or $W^{(l)}$ is needed.\n",
    "\n",
    "Next compute $Y^{(l)}$, the output of layer $l$ by activation through sigmoid.\n",
    "\n",
    "$Y^{(l)} = \\sigma(Z^{(l)})$\n",
    "\n",
    "The implemented feedforward algorithm should take in a NNet model and an input matrix $X$ and output the modified MLP model - the $Y$'s and $Z$'s computed should be stored in the model for the backpropagation algorithm.\n",
    "\n",
    "As usual, avoid for loops as much as possible, use the power of numpy. However, you may use a for loop to iterate through the layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c41a778a-0d06-4295-ac80-3ea6bfbad9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(model, X):\n",
    "\n",
    "    #TODO\n",
    "    A = X\n",
    "\n",
    "    for l in range(len(model.W)):\n",
    "        A_with_bias = np.vstack([A, np.ones((1, A.shape[1]))])  # Add bias term\n",
    "        Z = model.W[l] @ A_with_bias\n",
    "        model.Z[l][:, :Z.shape[1]] = Z\n",
    "\n",
    "        if l == len(model.W) - 1:\n",
    "            A = sigmoid(Z)\n",
    "        else:\n",
    "            A = ReLu(Z)\n",
    "        model.Y[l][:, :A.shape[1]] = A\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeea889-4e78-4bfc-b126-6d5d11ea571e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Question** (write answer in the cell below as a string)\n",
    "\n",
    "Explain what the bias term is and how adding a row of one's help with the bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecea6410-625d-4d82-8a17-8c00407fb7d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Write the answer here as a string:\n",
    "\n",
    "ans_bias = \"The bias term in a neural network is an extra parameter added to each neuron that allows the activation function to be shifted left or right, improving the model's flexibility. By adding a row of ones to the input (or activations), we can include the bias in the weight matrix multiplication, treating it as an additional weight. This makes the computation efficient and ensures each neuron has its own bias term, which helps the network learn patterns that do not pass through the origin.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b4c037-df9a-49eb-9519-51b86555b33c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q8.** Now we will implement the backpropagation algorithm. The cost function $C$ at the end is given by the square loss.\n",
    "\n",
    "$C = \\frac{1}{2} ||Y^{(L)} - Y||^{2}$, where $Y^{(L)}$ is the final output vector of the feedforward algorithm and $Y$ is the actual label vector associated with the input $X$.\n",
    "\n",
    "At each layer $l = 1, 2, \\cdots, L$ we compute the following (note that the gradients are matrices with the same dimensions as the variable to which we derivating with respect to):\n",
    "\n",
    "1. Gradient of $C$ with respect to $Z^{(l)}$ as <br> $\\frac{\\partial C}{\\partial Z^{(l)}} = deriv(A^{(l)}(Z^{(l)})) \\odot \\frac{\\partial C}{\\partial Y^{(L)}} $, <br> where $A^{(l)}$ is the activation function of the $l$th layer, and we use the derivative of that here. The $\\odot$ refers to the elementwise multiplication.\n",
    "\n",
    "2. Gradient of $C$ with respect to $W^{(l)}$ as <br> $\\frac{\\partial C}{\\partial W^{(l)}} = \\frac{\\partial C}{\\partial Z^{(l)}} (Y^{(l-1)})^{T}$ <br> this is entirely matrix multiplication.\n",
    "\n",
    "3. Gradient of $C$ with respect to $Y^{(l-1)}$ as <br> $\\frac{\\partial C}{\\partial Y^{(l-1)}} = (W^{(l)})^{T} \\frac{\\partial C}{\\partial Z^{(l)}}$ <br> this is also entirely matrix multiplication.\n",
    "\n",
    "4. Update weights by: <br> $W^{(l)} \\leftarrow W^{(l)} - \\eta \\frac{\\partial C}{\\partial W^{(l)}}$, <br> where $\\eta > 0$ is the learning rate.\n",
    "\n",
    "The loss derivative (the gradient of $C$ with respect to $Y^{(L)}$) at the last layer is given by:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial Y^{(L)}} = Y^{(L)} - Y$\n",
    "\n",
    "By convention we consider $Y^{(0)} = X$, the input data.\n",
    "\n",
    "Based on the backpropagation algorithm implement the backpropagation method in the following cell. Remember to temporarily add a row of ones to $Y^{(l-1)}$ (as the last row to be consistent with the grader) when computing $\\frac{\\partial C}{\\partial W^{(l)}}$ as we discussed back in the feedforward algorithm. Make sure you avoid for loops as much as possible.\n",
    "\n",
    "The function takes in a NNet model, input data $X$ and the corresponding class labels $Y$. learning rate can be set as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d958af3f-5a72-462d-844f-aba8ce5b03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, X, Y, eta = 0.01):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fad553-1ba4-4bee-b04b-249b9e04c6a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q9.** Now implement the training algorithm.\n",
    "\n",
    "The training method takes in training data $X$, actual label $Y$, number of epochs, batch_size, learning rate $\\eta > 0$. The training will happen in epochs. For each epoch, permute the data columns of both $X$ and $Y$, then divide both $X$ and $Y$ into mini batches each with the given batch size. Then run the feedforward and backpropagation for each such batch iteratively.\n",
    "\n",
    "At the end of each iteration, keep trach of the cost $C$ and the $l_2$-norm of change in each weight matrix $W^{(l)}$.\n",
    "\n",
    "At the end of the last epoch, plot the variation cost $C$ and change in weight matrices. Then return the trained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa4070ea-4a6b-4cba-b882-fa2ea79a9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NNet(X, Y, epochs = 20, batch_size = 1000, eta = 0.01):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd20299-a129-4d1d-a406-84e1a4dba621",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Section 3: Evaluation using test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d954c37c-f1b7-4e87-aedd-cf9e7624f612",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q10.** Implement the following function predict. Given test images (3 dimensional numpy array that contains $28 \\times 28$ digit images) it will correctly recognize the written digits between d1 and d2. You can assume that test_images will only contain images of digits d1 and d2. Inside predict you would need to preprocess the images using vectorize, predict the correct labels using model and then output the correct lables. Output should be a vector predicted of d1's and d2's, predicted[i] should correspond to the test_image[i]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "977dcbb8-1b5c-4f49-89f6-9bd90833957b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def predict(model, test_images, d1, d2):\n",
    "\n",
    "    #TODO\n",
    "    \n",
    "    return(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f1db0-8f5f-442c-84f1-578c77b99537",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q11.** Use the train_NNet function to train a MLP model to classify between images of digits $1$ and $5$. An accuracy $>= 99%$ is achievable. Test with different batch sizes, $\\eta$ values and hidden layers. Find which of those hyperparameters gives the best test accuracy. Name this model, model_1_5. This model will be tested on unseen test data within the grader. So make sure you train the best possible model. The grader will use your own predict function to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0561199-92cc-4344-8509-c8ab798c33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "\n",
    "\n",
    "model_1_5 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d0cb5-78d7-486e-98eb-1f162888c79a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q12.** Do the same as in Q11 with the digits $7$ and $9$ Name this model, model_7_9. This model will be tested on unseen test data within the grader. So make sure you train the best possible model. The grader will use your own predict function to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f341921-1e67-49d2-95c7-72c9f06486e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "\n",
    "\n",
    "model_7_9 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1973ff-0e69-451d-96b2-c591923e1653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_preprocess_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
