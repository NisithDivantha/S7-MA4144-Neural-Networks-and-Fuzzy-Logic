{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda0e25f-476a-43c0-b3e7-f96a707be566",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# <center>Recurrent Neural Networks</center>\n",
    "## <center>Inclass Project 3 - MA4144</center>\n",
    "\n",
    "This project contains multiple tasks to be completed, some require written answers. Questions that required written answers are given in blue fonts. Almost all written questions are open ended, they do not have a correct or wrong answer. You are free to give your opinions, but please provide related answers within the context.\n",
    "\n",
    "After finishing project run the entire notebook once and **save the notebook as a pdf** (File menu -> Save and Export Notebook As -> PDF). You are **required to upload both this ipynb file and the PDF on moodle**.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03930b71-f7aa-4eb8-a078-70fc89a1ac16",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Outline of the project\n",
    "\n",
    "The aim of the project is to build a RNN model to suggest autocompletion of half typed words. You may have seen this in many day today applications; typing an email, a text message etc. For example, suppose you type in the four letter \"univ\", the application may suggest you to autocomplete it by \"university\".\n",
    "\n",
    "![Autocomplete](https://d33v4339jhl8k0.cloudfront.net/docs/assets/5c12e83004286304a71d5b72/images/66d0cb106eb51e63b8f9fbc6/file-gBQe016VYt.gif)\n",
    "\n",
    "We will train a RNN to suggest possible autocompletes given $3$ - $4$ starting letters. That is if we input a string \"univ\" hopefully we expect to see an output like \"university\", \"universal\" etc.\n",
    "\n",
    "For this we will use a text file (wordlist.txt) containing 10,000 common English words (you'll find the file on the moodle link). The list of words will be the \"**vocabulary**\" for our model.\n",
    "\n",
    "We will use the Python **torch library** to implement our autocomplete model. \n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db6bc0-f7e0-473d-a172-e6579deea2ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Use the below cell to use any include any imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fdc286-3211-4a8f-9802-29d28a324bea",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622b61b-dba8-47bb-8e07-92b77e78f4fa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Section 1: Preparing the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555a82e5-e56c-4075-a2a2-071633cd4d4c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "WORD_SIZE = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f44ef-91d0-4d0e-afb5-a66240c9e1d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Instructions"
   },
   "source": [
    "**Q1.** In the following cell provide code to load the text file (each word is in a newline), then extract the words (in lowercase) into a list.\n",
    "\n",
    "For practical reasons of training the model we will only use words that are longer that $3$ letters and that have a maximum length of WORD_SIZE (this will be a constant we set at the beginning - you can change this and experiment with different WORD_SIZEs). As seen above it is set to $13$.\n",
    "\n",
    "So out of the extracted list of words filter out those words that match our criteria on word length.\n",
    "\n",
    "To train our model it is convenient to have words/strings of equal length. We will choose to convert every word to length of WORD_SIZE, by adding underscores to the end of the word if it is initially shorter than WORD_SIZE. For example, we will convert the word \"university\" (word length 10) into \"university___\" (wordlength 13). In your code include this conversion as well.\n",
    "\n",
    "Store the processed WORD_SIZE lengthed strings in a list called vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3219551c-a298-424a-a491-2d7f65a4ad6f",
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary prepared: 8847 words\n"
     ]
    }
   ],
   "source": [
    "# Load the wordlist.txt file and process the words\n",
    "with open('wordlist.txt', 'r') as file:\n",
    "    words = [line.strip().lower() for line in file.readlines()]\n",
    "\n",
    "# Filter words: longer than 3 letters and maximum length of WORD_SIZE\n",
    "filtered_words = [word for word in words if len(word) > 3 and len(word) <= WORD_SIZE]\n",
    "\n",
    "# Convert words to WORD_SIZE length by padding with underscores\n",
    "vocab = [word + '_' * (WORD_SIZE - len(word)) for word in filtered_words]\n",
    "\n",
    "print(f\"Vocabulary prepared: {len(vocab)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3a6fe-c0a7-4808-aa1a-4c3ad6db6de3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<font color='blue'>In the above explanation it was mentioned \"for practical reasons of training the model we will only use words that are longer that $3$ letters and that have a certain maximum length\". In your opinion what could be those practical? Will hit help to build a better model?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf86cdd-96b2-40eb-8230-c3f04e93cfc4",
   "metadata": {
    "deletable": false,
    "editable": true,
    "id": "Ans1",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Answer** \n",
    "\n",
    "The practical reasons for using words longer than 3 letters and with maximum length limitations are:\n",
    "\n",
    "1. **Training Efficiency**: Very short words (â‰¤3 letters) don't provide enough context for meaningful autocompletion. For example, \"cat\" or \"dog\" are already complete and don't need autocompletion.\n",
    "\n",
    "2. **Memory and Computational Constraints**: Very long words require more memory and computational resources. By limiting to WORD_SIZE (13), we keep the model manageable while still covering most common English words.\n",
    "\n",
    "3. **Consistent Input Size**: Having a fixed maximum length allows for efficient batch processing during training. All words are padded to the same length, making tensor operations faster and more straightforward.\n",
    "\n",
    "4. **Realistic Use Case**: In real applications, autocompletion is most useful for moderately long words (4-13 letters). Users typically don't need help with very short words, and extremely long words are rare.\n",
    "\n",
    "This approach will help build a better model because:\n",
    "- It focuses on the most relevant use cases for autocompletion\n",
    "- Reduces noise from trivial cases (very short words)\n",
    "- Enables efficient batch training with uniform input sizes\n",
    "- Balances model complexity with practical utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b2e6d-f771-4782-8b21-73c121565faa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q2** To input words into the model, we will need to convert each letter/character into a number. as we have seen above, the only characters in our list vocab will be the underscore and lowercase english letters. so we will convert these $27$ characters into numbers as follows: underscore -> $0$, 'a' -> $1$, 'b' -> $2$, $\\cdots$, 'z' -> $26$. In the following cell,\n",
    "\n",
    "(i) Implement a method called char_to_num, that takes in a valid character and outputs its numerical assignment.\n",
    "\n",
    "(ii) Implement a method called num_to_char, that takes in a valid number from $0$ to $26$ and outputs the corresponding character.\n",
    "\n",
    "(iii) Implement a method called word_to_numlist, that takes in a word from our vocabulary and outputs a (torch) tensor of numbers that corresponds to each character in the word in that order. For example: the word \"united_______\" will be converted to tensor([21, 14,  9, 20,  5,  4,  0,  0,  0,  0,  0,  0,  0]). You are encouraged to use your char_to_num method for this.\n",
    "\n",
    "(iv) Implement a method called numlist_to_word, that does the opposite of the above described word_to_numlist, given a tensor of numbers from $0$ to $26$, outputs the corresponding word. You are encouraged to use your  num_to_char method for this.\n",
    "\n",
    "Note: As mentioned since we are using the torch library we will be using tensors instead of the usual python lists or numpy arrays. Tensors are the list equivalent in torch. Torch models only accept tensors as input and they output tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "054a4ab4-5883-4948-adc5-eb8916b6234d",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def char_to_num(char):\n",
    "    \"\"\"Convert character to number: '_' -> 0, 'a' -> 1, 'b' -> 2, ..., 'z' -> 26\"\"\"\n",
    "    if char == '_':\n",
    "        num = 0\n",
    "    else:\n",
    "        # Convert 'a' to 1, 'b' to 2, etc.\n",
    "        num = ord(char) - ord('a') + 1\n",
    "    return num\n",
    "\n",
    "def num_to_char(num):\n",
    "    \"\"\"Convert number to character: 0 -> '_', 1 -> 'a', 2 -> 'b', ..., 26 -> 'z'\"\"\"\n",
    "    if num == 0:\n",
    "        char = '_'\n",
    "    else:\n",
    "        # Convert 1 to 'a', 2 to 'b', etc.\n",
    "        char = chr(num - 1 + ord('a'))\n",
    "    return char\n",
    "\n",
    "def word_to_numlist(word):\n",
    "    \"\"\"Convert word to tensor of numbers corresponding to each character\"\"\"\n",
    "    num_list = []\n",
    "    for char in word:\n",
    "        num_list.append(char_to_num(char))\n",
    "    numlist = torch.tensor(num_list)\n",
    "    return numlist\n",
    "\n",
    "def numlist_to_word(numlist):\n",
    "    \"\"\"Convert tensor of numbers back to word\"\"\"\n",
    "    word = \"\"\n",
    "    for num in numlist:\n",
    "        word += num_to_char(int(num))\n",
    "    return word\n",
    "\n",
    "# Utility functions ready for use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d1936-fadb-4ddb-9027-3a75960aa6b1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font color='blue'>We convert letter into just numbers based on their aphabetical order, I claim that it is a very bad way to encode data such as letters to be fed into learning models, please write your explanation to or against my claim. If you are searching for reasons, the keyword 'categorical data' may be useful. Although the letters in our case are not treated as categorical data, the same reasons as for categorical data is applicable. Even if my claim is valid, at the end it won't matter due to something called \"embedding layers\" that we will use in our model. What is an embedding layer? What is it's purpose? Explain.</font> (write answers in the cell below as a string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e29cb406-1f95-46c3-a1e2-d13fe0ed557b",
   "metadata": {
    "deletable": false,
    "editable": true,
    "id": "Ans2",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write answer here as strings:\n",
    "\n",
    "answer = \"\"\"\n",
    "I agree with the claim that converting letters to numbers based on alphabetical order is a problematic way to encode data for learning models. Here's why:\n",
    "\n",
    "PROBLEMS WITH ORDINAL ENCODING OF LETTERS:\n",
    "1. Implies False Ordering: The encoding suggests that 'b' (2) is \"greater than\" 'a' (1) and \"less than\" 'c' (3), which is meaningless for letters. Letters don't have a natural numerical relationship.\n",
    "\n",
    "2. Artificial Distance Relationships: The model might incorrectly learn that 'a' and 'b' are \"closer\" than 'a' and 'z', when in reality all letters should be treated as equally different categories.\n",
    "\n",
    "3. Categorical Data Nature: Letters are categorical data - they represent distinct categories without inherent order. Categorical data should typically be encoded using one-hot encoding or similar methods that don't impose false relationships.\n",
    "\n",
    "EMBEDDING LAYERS - THE SOLUTION:\n",
    "An embedding layer is a learnable lookup table that maps discrete tokens (like our character numbers 0-26) to dense vector representations.\n",
    "\n",
    "PURPOSE OF EMBEDDING LAYERS:\n",
    "1. Learned Representations: Instead of using arbitrary numbers, the model learns meaningful vector representations for each character during training.\n",
    "\n",
    "2. Dense Vectors: Each character gets represented as a dense vector (e.g., 50-dimensional) rather than a sparse one-hot vector or meaningless integer.\n",
    "\n",
    "3. Contextual Meaning: The embedding vectors capture semantic relationships. Characters that often appear in similar contexts will have similar embeddings.\n",
    "\n",
    "4. Efficiency: More memory and computationally efficient than one-hot encoding for large vocabularies.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92070a74-0f42-435d-a3ba-b38f0d1aaf3c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Section 2: Implementing the Autocomplete model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb965e-afcd-41ae-86f0-2f3d4682e18b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will implement a RNN LSTM model. The [video tutorial](https://www.youtube.com/watch?v=tL5puCeDr-o) will be useful. Our model will be only one hidden layer, but feel free to sophisticate with more layers after the project for your own experiments.\n",
    "\n",
    "Our model will contain all the training and prediction methods as single package in a class (autocompleteModel) we will define and implement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfe311c-669d-4d58-a833-ae3970b6d271",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.002  # Optimized for large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4976fc91-2c4e-497a-954e-9014dd31be5e",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class autocompleteModel(nn.Module):\n",
    "\n",
    "    #Constructor\n",
    "    def __init__(self, alphabet_size, embed_dim, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        #Set the input parameters to self parameters\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #Initialize the layers in the model:\n",
    "        #1 embedding layer, 1 - LSTM cell (hidden layer), 1 fully connected layer with linear activation\n",
    "        self.embedding = nn.Embedding(alphabet_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, alphabet_size)\n",
    "\n",
    "    #Feedforward\n",
    "    def forward(self, character, hidden_state, cell_state):\n",
    "\n",
    "        #Perform feedforward in order\n",
    "        #1. Embed the input (one charcter represented by a number)\n",
    "        #2. Feed the embedded output to the LSTM cell\n",
    "        #3. Feed the LSTM output to the fully connected layer to obtain the output\n",
    "        #4. return the output, and both the hidden state and cell state from the LSTM cell output\n",
    "\n",
    "        # Ensure character is the right shape for embedding (batch_size=1, seq_len=1)\n",
    "        if character.dim() == 0:  # scalar\n",
    "            character = character.unsqueeze(0).unsqueeze(0)\n",
    "        elif character.dim() == 1:  # 1D tensor\n",
    "            character = character.unsqueeze(0)\n",
    "        \n",
    "        # 1. Embed the input\n",
    "        embedded = self.embedding(character)\n",
    "        \n",
    "        # 2. Feed to LSTM\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(embedded, (hidden_state, cell_state))\n",
    "        \n",
    "        # 3. Feed to fully connected layer\n",
    "        output = self.fc(lstm_out.squeeze(0).squeeze(0))\n",
    "        \n",
    "        return output, hidden_state, cell_state\n",
    "\n",
    "    #Intialize the first hidden state and cell state (for the start of a word) as zero tensors of required length.\n",
    "    def initial_state(self):\n",
    "        h0 = torch.zeros(self.num_layers, 1, self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, 1, self.hidden_size)\n",
    "        return (h0, c0)\n",
    "\n",
    "    #Train the model in epochs given the vocab, the training will be fed in batches of batch_size\n",
    "    def trainModel(self, vocab, epochs = 5, batch_size = 100):\n",
    "\n",
    "        #Convert the model into train mode\n",
    "        self.train()\n",
    "\n",
    "        #Set the optimizer (ADAM), you may need to provide the model parameters and learning rate\n",
    "        optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        #Keep a log of the loss at the end of each training cycle.\n",
    "        loss_log = []\n",
    "        \n",
    "        # Calculate number of iterations per epoch\n",
    "        num_iter = len(vocab) // batch_size\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "            # Shuffle the vocab list at the start of each epoch\n",
    "            random.shuffle(vocab)\n",
    "\n",
    "            for i in range(num_iter):\n",
    "\n",
    "                # Set the loss to zero, initialize the optimizer with zero_grad at the beginning of each training cycle.\n",
    "                optimizer.zero_grad()\n",
    "                loss = 0\n",
    "\n",
    "                # Get batch\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, len(vocab))\n",
    "                vocab_batch = vocab[start_idx:end_idx]\n",
    "\n",
    "                for word in vocab_batch:\n",
    "\n",
    "                    # Initialize the hidden state and cell state at the start of each word.\n",
    "                    hidden_state, cell_state = self.initial_state()\n",
    "\n",
    "                    # Convert the word into a tensor of number and create input and target from the word\n",
    "                    # Input will be the first WORD_SIZE - 1 characters and target is the last WORD_SIZE - 1 characters\n",
    "                    word_nums = word_to_numlist(word)\n",
    "                    inputs = word_nums[:-1]  # First WORD_SIZE - 1 characters\n",
    "                    targets = word_nums[1:]  # Last WORD_SIZE - 1 characters (shifted by 1)\n",
    "\n",
    "                    # Loop through each character (as a number) in the word\n",
    "                    for c in range(WORD_SIZE - 1):\n",
    "                        # Feed the cth character to the model (feedforward) and compute the loss (use cross entropy in torch)\n",
    "                        output, hidden_state, cell_state = self.forward(inputs[c], hidden_state, cell_state)\n",
    "                        loss += nn.CrossEntropyLoss()(output.unsqueeze(0), targets[c].unsqueeze(0))\n",
    "\n",
    "                # Compute the average loss per word in the batch and perform backpropagation (.backward())\n",
    "                loss = loss / len(vocab_batch)\n",
    "                loss.backward()\n",
    "                    \n",
    "                # Update model parameters using the optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the loss_log \n",
    "                loss_log.append(loss.item())\n",
    "\n",
    "            if e % 3 == 0:  # Print every 3 epochs\n",
    "                print(f\"Epoch: {e+1}, Average Loss: {sum(loss_log[-num_iter:]) / num_iter:.4f}\")\n",
    "\n",
    "        # Plot training progress\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(loss_log)\n",
    "        plt.title('Training Loss Over Time')\n",
    "        plt.xlabel('Training Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Softmax function (helper function)\n",
    "    def softmax(self, x, temperature=1.0):\n",
    "        \"\"\"Apply softmax with optional temperature scaling\"\"\"\n",
    "        x = x / temperature\n",
    "        exp_x = torch.exp(x - torch.max(x))  # Subtract max for numerical stability\n",
    "        return exp_x / torch.sum(exp_x)\n",
    "\n",
    "    # Perform autocompletion given a sample of strings (typically 3-5 starting letters)\n",
    "    def autocomplete(self, sample, temperature=1.0, max_completions=5):\n",
    "\n",
    "        # Convert the model into evaluation mode\n",
    "        self.eval()\n",
    "        completed_list = []\n",
    "\n",
    "        # In the following loop for each sample item initialize hidden and cell states, then predict the remaining characters\n",
    "        # You will have to convert the output into a softmax probability distribution, then use torch.multinomial \n",
    "        for literal in sample:\n",
    "            completions = []\n",
    "            \n",
    "            # Generate multiple completions for variety\n",
    "            for _ in range(max_completions):\n",
    "                # Initialize hidden and cell states\n",
    "                hidden_state, cell_state = self.initial_state()\n",
    "                \n",
    "                # Convert the partial word to numbers and pad to WORD_SIZE\n",
    "                partial_padded = literal + '_' * (WORD_SIZE - len(literal))\n",
    "                current_word = list(partial_padded)\n",
    "                \n",
    "                # Feed the existing characters through the network\n",
    "                with torch.no_grad():\n",
    "                    for i in range(len(literal)):\n",
    "                        char_num = torch.tensor(char_to_num(current_word[i]))\n",
    "                        output, hidden_state, cell_state = self.forward(char_num, hidden_state, cell_state)\n",
    "                    \n",
    "                    # Generate the remaining characters\n",
    "                    for i in range(len(literal), WORD_SIZE):\n",
    "                        # Get probability distribution using softmax\n",
    "                        probs = self.softmax(output, temperature)\n",
    "                        \n",
    "                        # Sample from the distribution\n",
    "                        next_char_num = torch.multinomial(probs, 1).item()\n",
    "                        \n",
    "                        # Convert back to character and update current word\n",
    "                        next_char = num_to_char(next_char_num)\n",
    "                        current_word[i] = next_char\n",
    "                        \n",
    "                        # If we hit underscore, we can stop (word is complete)\n",
    "                        if next_char == '_':\n",
    "                            break\n",
    "                        \n",
    "                        # Feed this character to get next prediction\n",
    "                        output, hidden_state, cell_state = self.forward(torch.tensor(next_char_num), hidden_state, cell_state)\n",
    "                \n",
    "                # Convert back to word and clean up\n",
    "                completed_word = ''.join(current_word).rstrip('_')\n",
    "                if completed_word not in completions and len(completed_word) > len(literal):\n",
    "                    completions.append(completed_word)\n",
    "            \n",
    "            # Remove duplicates and take best completions\n",
    "            unique_completions = list(set(completions))[:3]  # Top 3 unique completions\n",
    "            completed_list.append({\n",
    "                'input': literal,\n",
    "                'completions': unique_completions if unique_completions else [literal + '...']\n",
    "            })\n",
    "\n",
    "        return completed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a279180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model implementation ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fddb9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training optimized autocomplete model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[32m     13\u001b[39m best_model = autocompleteModel(alphabet_size, embed_dim, hidden_size, num_layers)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m best_model.trainModel(vocab, epochs=epochs, batch_size=batch_size)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mautocompleteModel.trainModel\u001b[39m\u001b[34m(self, vocab, epochs, batch_size)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Compute the average loss per word in the batch and perform backpropagation (.backward())\u001b[39;00m\n\u001b[32m    100\u001b[39m loss = loss / \u001b[38;5;28mlen\u001b[39m(vocab_batch)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m loss.backward()\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Update model parameters using the optimizer\u001b[39;00m\n\u001b[32m    104\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/data_preprocess_env/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m torch.autograd.backward(\n\u001b[32m    648\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    649\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/data_preprocess_env/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m _engine_run_backward(\n\u001b[32m    355\u001b[39m     tensors,\n\u001b[32m    356\u001b[39m     grad_tensors_,\n\u001b[32m    357\u001b[39m     retain_graph,\n\u001b[32m    358\u001b[39m     create_graph,\n\u001b[32m    359\u001b[39m     inputs_tuple,\n\u001b[32m    360\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    361\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    362\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/data_preprocess_env/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    830\u001b[39m         t_outputs, *args, **kwargs\n\u001b[32m    831\u001b[39m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train the best model configuration (Large Model)\n",
    "print(\"Training optimized autocomplete model...\")\n",
    "\n",
    "# Best configuration parameters\n",
    "alphabet_size = 27\n",
    "embed_dim = 128\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "epochs = 12\n",
    "batch_size = 80\n",
    "\n",
    "# Create and train the model\n",
    "best_model = autocompleteModel(alphabet_size, embed_dim, hidden_size, num_layers)\n",
    "best_model.trainModel(vocab, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b5489-b770-4519-b20c-4f2beebfb8f9",
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "source": [
    "## Section 3: Using and evaluating the model\n",
    "\n",
    "(i) Initialize and train autocompleteModels using different embedding dimensions and hidden layer sizes. Use different learning rates, epochs, batch sizes. Train the best model you can.\n",
    "\n",
    "(ii) Evaluate it on different samples of partially filled in words to test your model. Eg: [\"univ\", \"math\", \"neur\", \"engin\"] etc.\n",
    "\n",
    "(iii) Set your best model, to the variable best_model. This model will be tested against random inputs (3-4 starting strings of common English words). **This will be the main contributor for your score in this project**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac2ab12-0cc8-48d1-816f-ab8ec7ac23cd",
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation on test samples:\n",
      "  'univ' â†’ ['univisis', 'universations', 'unives']\n",
      "  'math' â†’ ['mather', 'matheras', 'mathors']\n",
      "  'neur' â†’ ['neurs', 'neurtion', 'neurbay']\n",
      "  'engin' â†’ ['engine', 'engines']\n",
      "  'comp' â†’ ['complicated', 'complet', 'complied']\n",
      "\n",
      "Model ready for autocompletion tasks!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model\n",
    "test_samples = [\"univ\", \"math\", \"neur\", \"engin\", \"comp\", \"prog\", \"algo\", \"data\", \"mach\", \"deep\"]\n",
    "results = best_model.autocomplete(test_samples, temperature=0.6, max_completions=3)\n",
    "\n",
    "print(\"Model evaluation on test samples:\")\n",
    "for result in results[:5]:\n",
    "    print(f\"  '{result['input']}' â†’ {result['completions']}\")\n",
    "\n",
    "print(\"\\nModel ready for autocompletion tasks!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_preprocess_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
