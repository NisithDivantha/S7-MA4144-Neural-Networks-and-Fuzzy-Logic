{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4b7cc4-4743-4555-ba9d-31856132f962",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# <center>ML Basics </center>\n",
    "## <center>Inclass Project 1 - MA4144</center>\n",
    "\n",
    "This project contains 12 tasks/questions to be completed.\n",
    "\n",
    "After finishing project run the entire notebook once and **save the notebook as a pdf** (File menu -> Save and Export Notebook As -> PDF). You are **required to upload this PDF on moodle**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc927cf7-fbea-4684-b942-8a9405400822",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Use this cell to use any include any imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61792cd0-155b-436c-8ebe-82933548d92d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saroa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3f445-ab74-4697-86b9-8ffec429d923",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Section 1\n",
    "\n",
    "In this section we will analyse a dataset to look into its statistical propeties. For this we will use the DiabetesTrain.csv dataset. Each row corresponds to a single patient. The first 8 columns correspond to the features of the patients that may help predict risk of diabetes. The outcome column is a binary column represting the risk of diabetes, outcome 1 : high risk of diabetes and outcome 0 little to no risk of diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a950e4d-c397-47a9-b041-46ba72d5d95c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q1.** Read the dataset into a pandas dataframe called diabetesData. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a4ad00-0298-4de0-b952-de1ac4990e7d",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "diabetesData = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da24345f-cb2b-437b-af61-9d0067fd56af",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q2.** Let us define the following events.\n",
    "\n",
    "A = Patient has BMI less than 25\n",
    "\n",
    "B = Patient has Glucose level greater than 100\n",
    "\n",
    "C = Patient has had more than 2 pregnancies\n",
    "\n",
    "D = Patient has high risk of diabetes\n",
    "\n",
    "Based on the above definitions determine the following probabilities. Pandas dataframe inbuilt functions such as count, group by, would be useful for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63382318-9b8d-4ea3-a74c-e3ac6ebbbc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "#P(A)\n",
    "P_A = None\n",
    "\n",
    "#P(B)\n",
    "P_B = None\n",
    "\n",
    "#P(C)\n",
    "P_C = None\n",
    "\n",
    "#P(D)\n",
    "P_D = None\n",
    "\n",
    "#P(A, D)\n",
    "P_A_D = None\n",
    "\n",
    "#P(B, D)\n",
    "P_B_D = None\n",
    "\n",
    "#P(C, D)\n",
    "P_C_D = None\n",
    "\n",
    "#Indicate which one out of A, B, C contributes the most towards high risk of diabetes.\n",
    "#Assign one of 'A', 'B', 'C' to the following variable Q2, indicating your answer.\n",
    "#Hint: Compute the necessary conditional probabilities and then compare.\n",
    "Q2 = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641a2bf-7add-415a-ae07-8e8d8f55fbcc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q3.** Now we will compute the covariance and correlation matrices from scratch. For this do not use any inbuilt functions. Follow the steps outlined below. Each step is graded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ca493-d8c1-493b-a4f4-7e8f1cddc382",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step1:** Convert the diabetesData dataframe into a 2-dimensional numpy array with the same number of rows and columns as in the dataframe. Name it diabetesX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147f70c4-d19d-4e77-b69b-170bc7da8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "diabetesX = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33652ee2-79f0-4908-9c97-7f8a22ce9c51",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step2:** In diabetesX; center every column, by subtracting each column by the column mean and reassign it to diabetesX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a106986-e334-49ea-9259-9b2f53d4f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "#After centering\n",
    "diabetesX = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd1da1-6bfd-4aac-a80a-24226faf45da",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step3:** Compute the covariance matrix. Use, matrix operations in numpy such as matrix multiplication, matrix transpose and don't forget to average. Assign it to the variable cov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06799e49-de01-424a-a660-1503ac9a023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "cov = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55b04f-6ee0-4729-85ab-b0d798114ce7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step4:** Compute the matrix varmat, whose $(i, j)$ entry is $\\sqrt{var(i)var(j)}$, where $var(k)$ is the variance of the $k$th column of the diabetesX matrix. The varinces can be extracted from the covariance matrix itself appropriately and varmat can be computed by appropriate matrix multiplication of a column matrix and a row matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aabd144a-f091-4b1d-bf06-03fc574f8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "varmat = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33ea08-3037-45b1-918f-d0c737fc3725",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step5:** Use the cov matrix and varmat matrix appropriately to compute the correlational matrix corr. And then use seaborn (sns imported above) to plot an annotated heatmap of the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fc14db-d7f2-45dc-8ceb-919903ece94e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "corr = None\n",
    "\n",
    "#Plot the heatmap.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dabb96-e161-4c79-b8c5-c41d29fcb014",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "From the heatmap read the following correlations. Also, answer the question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14101511-4957-4665-a712-4393e3c38355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corr(BMI, Outcome)\n",
    "Corr1 = None\n",
    "\n",
    "#Corr(Glucose, Outcome)\n",
    "Corr2 = None\n",
    "\n",
    "#Corr(Pregnancies, Outcome)\n",
    "Corr3 = None\n",
    "\n",
    "#Out of the 8 features, which two features are the most correlated. Fill in the list variable bestcorr\n",
    "bestcorr = ['', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b1e02-f080-4392-9381-608124de444f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Section 2\n",
    "\n",
    "In this section we will train a logistic regression model on the diabetes dataset to predict the risk of diabetes given patient data. We will then use it to perform predictions on previously unseen (test) data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b5fdb-2e48-427f-afde-876a8c9b53a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q4.** Preprocess Data.\n",
    " \n",
    "Implement a function normalizeData that normalizes each column of a data array. Recall that a column $x$ is normalized by $z = \\frac{x - mean(x)}{std(x)}$. The function should return the normalized data matrix and the mean and standard deviations of each column (we'll need these to normalize the test data later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1819fc2-886d-4430-9e4a-bb5d6ef2b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(X, mean = np.array([]), std = np.array([])):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    # Implement such that if the mean and std are empty arrays then mean and std is calculated here, else use the mean and std passed in as parameters\n",
    "    mean = None\n",
    "    std = None \n",
    "    \n",
    "    normalized_X = None\n",
    "\n",
    "    return normalized_X, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0477700-8a5b-468f-b926-0cc0ad8d2100",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q5.** Sigmoid function and it's derivative.\n",
    "\n",
    "1. Implement the sigmoid function. Given a numpy array, compute the sigmoid values of the array. It should return an array of sigmoid values. If you see any overflow errors/warnings popping up, that is because the numbers coming out of the exponential function $e^{-t}$ in the sigmoid function are too large to handle. In that case, please clip the input between some large enough negative and positive value (look into numpy.clip).\n",
    "2. Implement the derivative of the sigmoid function. Recall that if, $p = \\sigma(t)$, then $p' = p(1-p)$. The function should take in an array of $p$ values and then return the array of $p'$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7e3bf14-35f5-4f55-b73a-5b44f172c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid function\n",
    "def sigmoid(t):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    sig = None\n",
    "    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77204775-8069-4447-a569-b04245b9375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derivative of sigmoid function\n",
    "def derivSigmoid(p):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    deriv_p = None\n",
    "\n",
    "    return deriv_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8406b3-543e-4ef5-8509-7dea6097593b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q6.** Compute sigmoid probabilities.\n",
    "\n",
    "Recall that $p = P(y = 1 | x) = \\sigma(x^T \\omega + b)$, where $\\omega = \\begin{bmatrix} \\omega_1 \\\\ \\omega_2 \\\\ \\vdots \\\\ \\omega_d \\end{bmatrix}$ are the weights and $b$ is the bias term. Implement a function sigProg to calculate those probabilities, given a matrix $X$ of data, weight vector $\\omega$ and bias $b$. It is possible to compute the array of probabilities for the entire dataset at once without a single for-loop, by just using matrix mutltiplications, which is the rightway to achieve maximum efficiency. You are supposed to use the above implemented sigmoid function. This should return an array of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7184fb8-9f8d-4e4d-a8a5-64c28a529475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigProg(X, w, b):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4c65d-042a-46d7-a59d-e57554bafa4a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q7.** Compute loss gradient.\n",
    "\n",
    "Recall that the loss function for logistic regression is given by,\n",
    "\n",
    "$L(\\omega, b) = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} (p_i - y_i)^2 + \\lambda reg(\\omega)$ where is the sigmoid probability $p_i = P(y_i = 1 | x_i)$ for the data point $(x_i, y_i)$ and $reg(\\omega)$ is the regularization term - it could be either ridge $reg(\\omega) = ||\\omega||_2^2$ or lasso $reg(\\omega) = ||\\omega||_1$ or no regularization at all. $\\lambda \\geq 0$ is the regularization constant. $N$ is the number of datapoints.\n",
    "\n",
    "Then, the gradient of the loss function with respect to $\\omega$ and the derivative with respect to $b$ is given by,\n",
    "\n",
    "$\\nabla_{\\omega}L = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} (p_i - y_i) p'_i x_i \\ + \\ \\lambda reg'(\\omega)$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} (p_i - y_i) p'_i$\n",
    "\n",
    "Here,\n",
    "\n",
    "$reg'(\\omega) = \\begin{cases} \\omega \\; &\\textrm{ if ridge } \\\\ sign(\\omega) \\; &\\textrm{ if lasso } \\end{cases}$\n",
    "\n",
    "Implement a function named gradient to compute the gradient and derivative of the loss function, given data matrix $X$, output vector $y$, weight vector $\\omega$ and bias $b$. The function should also be able to take in other parameters such as reg (= \"none\", \"ridge\" or \"lasso\") and lambda. It should return the gradient and derivative. All these computations can be done without a single for loop, only by using numpy builtins for matrix computations, which is the most desired way to achieve efficiency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6cc7e1-0f7c-408e-84de-442e53d19ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, w, b, reg = \"none\", Lambda = 0.1):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    grad_w = None\n",
    "    deriv_b = None\n",
    "\n",
    "    return grad_w, deriv_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98327e0-e8ed-43b4-89a8-c1fb1767d274",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q8.** Gradient descent algorithm.\n",
    "\n",
    "Recall the following update rule for gradient descent,\n",
    "\n",
    "$\\omega \\leftarrow \\omega - \\eta \\nabla_{\\omega} L$\n",
    "\n",
    "$b \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "where $\\eta > 0$ is the learning rate.\n",
    "\n",
    "Implement the gradient descent algorithm in a function grad_descent given the gradient vector $\\nabla_{\\omega} L$ (grad_w), derivative $\\frac{\\partial L}{\\partial b}$ (deriv_b), weights $\\omega$ and bias $b$. Also should be able to accept the learning rate $\\eta$ (eta). The function should return the updated $\\omega$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bea3ddb7-6adb-4a26-a720-bb7c442ef8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(grad_w, deriv_b, w, b, eta = 0.01):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce15b6-54fe-40ac-862a-8e5b4d06584d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q9.** Train model\n",
    "\n",
    "Implement the function train. It should initialize $\\omega$ and $b$ to some random values or zeros. Then interatively update the $\\omega$ and $b$ using gradient descent, untill the number of interations reach the maximum number of interations specified as max_iter. The function will take in training data $(X, y)$, regularization details (type of reg, and $\\lambda$), and learning rate $\\eta$ (eta). Finally, it should return the trained $\\omega$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3830aba7-f4f4-4f91-8098-73b9599571b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, reg = 'none', Lambda = 0.1, eta = 0.01, max_iter = 1000):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    w = None\n",
    "    b = None\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586151b-5edc-4fe1-b34f-b51f2f01b4d6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q10.** Predict using the model\n",
    "\n",
    "Implement a function predict to output predictions for given input data $X$, trained weights $\\omega$ and $b$. Make sure that the output should only consist of $1$'s and $0$'s. The function should return the predictions $\\hat{y}$ (yhat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5db246b-cd53-4593-bc5a-4c7edefa8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    yhat = None\n",
    "\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4434d-102b-423c-a617-c208f5ae6654",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q11.** Use the above logistic regression algorithm on the diabetes dataset. Train a model ($\\omega$, $b$). Then evaluate it. Use classification error to evaluate it.\n",
    "\n",
    "Start by separating the diabetesData into input (features) and output (Outcome), and store them in numpy matrices X_train and y_train respectively, and then normalizing the input features.\n",
    "\n",
    "Some tips to improve accuracy.\n",
    "\n",
    "1. Use cross validation to find the best hyperparameters $\\eta$, $\\lambda$, and regularization type.\n",
    "2. Plot the loss function versus iterations while training (you can do this by additionally collecting the loss values within the train function), having a smooth decreasing graph will indicate proper training.\n",
    "3. As above plot the $\\ell_2$-norm of the $\\omega$ vector versus iterations, we expect smooth decreasing curve for this as well.\n",
    "\n",
    "More tips and additional clarifications in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f677040-50b3-47ad-a5f8-9dc6b597dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "X_train = None\n",
    "y_train = None\n",
    "\n",
    "#Train and evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c2edf-e4f4-45ea-8efd-47eb11b1a8ca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q12.** Testing on previously unseen test data - final part.\n",
    "\n",
    "1. Load the DiabetesTest.csv file.\n",
    "2. Use your best trained model to predict the 'Outcome' for this test data, assign your predictions to y_test_pred, this should be an array of $0$'s and $1$'s.\n",
    "3. This will be graded on the level of accuracy of your predictions.\n",
    "4. Remember to properly normalize your data before using them in the model. For normalization use the mean and standard deviation of the training data not the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75e6055e-00ce-48eb-bf80-fb1a391f9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "y_test_pred = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
